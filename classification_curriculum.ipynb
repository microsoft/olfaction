{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def load_model(exp_configure):\n",
    "    if exp_configure['model'] == 'GCN':\n",
    "        from dgllife.model import GCNPredictor\n",
    "        model = GCNPredictor(\n",
    "            in_feats=exp_configure['in_node_feats'],\n",
    "            hidden_feats=[exp_configure['gnn_hidden_feats']] * exp_configure['num_gnn_layers'],\n",
    "            activation=[F.relu] * exp_configure['num_gnn_layers'],\n",
    "            residual=[exp_configure['residual']] * exp_configure['num_gnn_layers'],\n",
    "            batchnorm=[exp_configure['batchnorm']] * exp_configure['num_gnn_layers'],\n",
    "            dropout=[exp_configure['dropout']] * exp_configure['num_gnn_layers'],\n",
    "            predictor_hidden_feats=exp_configure['predictor_hidden_feats'],\n",
    "            predictor_dropout=exp_configure['dropout'],\n",
    "            n_tasks=exp_configure['n_tasks'])\n",
    "    elif exp_configure['model'] == 'GAT':\n",
    "        from dgllife.model import GATPredictor\n",
    "        model = GATPredictor(\n",
    "            in_feats=exp_configure['in_node_feats'],\n",
    "            hidden_feats=[exp_configure['gnn_hidden_feats']] * exp_configure['num_gnn_layers'],\n",
    "            num_heads=[exp_configure['num_heads']] * exp_configure['num_gnn_layers'],\n",
    "            feat_drops=[exp_configure['dropout']] * exp_configure['num_gnn_layers'],\n",
    "            attn_drops=[exp_configure['dropout']] * exp_configure['num_gnn_layers'],\n",
    "            alphas=[exp_configure['alpha']] * exp_configure['num_gnn_layers'],\n",
    "            residuals=[exp_configure['residual']] * exp_configure['num_gnn_layers'],\n",
    "            predictor_hidden_feats=exp_configure['predictor_hidden_feats'],\n",
    "            predictor_dropout=exp_configure['dropout'],\n",
    "            n_tasks=exp_configure['n_tasks']\n",
    "        )\n",
    "    elif exp_configure['model'] == 'Weave':\n",
    "        from dgllife.model import WeavePredictor\n",
    "        model = WeavePredictor(\n",
    "            node_in_feats=exp_configure['in_node_feats'],\n",
    "            edge_in_feats=exp_configure['in_edge_feats'],\n",
    "            num_gnn_layers=exp_configure['num_gnn_layers'],\n",
    "            gnn_hidden_feats=exp_configure['gnn_hidden_feats'],\n",
    "            graph_feats=exp_configure['graph_feats'],\n",
    "            gaussian_expand=exp_configure['gaussian_expand'],\n",
    "            n_tasks=exp_configure['n_tasks']\n",
    "        )\n",
    "    elif exp_configure['model'] == 'MPNN':\n",
    "        from dgllife.model import MPNNPredictor\n",
    "        model = MPNNPredictor(\n",
    "            node_in_feats=exp_configure['in_node_feats'],\n",
    "            edge_in_feats=exp_configure['in_edge_feats'],\n",
    "            node_out_feats=exp_configure['node_out_feats'],\n",
    "            edge_hidden_feats=exp_configure['edge_hidden_feats'],\n",
    "            num_step_message_passing=exp_configure['num_step_message_passing'],\n",
    "            num_step_set2set=exp_configure['num_step_set2set'],\n",
    "            num_layer_set2set=exp_configure['num_layer_set2set'],\n",
    "            n_tasks=exp_configure['n_tasks']\n",
    "        )\n",
    "    elif exp_configure['model'] == 'AttentiveFP':\n",
    "        from dgllife.model import AttentiveFPPredictor\n",
    "        model = AttentiveFPPredictor(\n",
    "            node_feat_size=exp_configure['in_node_feats'],\n",
    "            edge_feat_size=exp_configure['in_edge_feats'],\n",
    "            num_layers=exp_configure['num_layers'],\n",
    "            num_timesteps=exp_configure['num_timesteps'],\n",
    "            graph_feat_size=exp_configure['graph_feat_size'],\n",
    "            dropout=exp_configure['dropout'],\n",
    "            n_tasks=exp_configure['n_tasks']\n",
    "        )\n",
    "    elif exp_configure['model'] in ['gin_supervised_contextpred', 'gin_supervised_infomax',\n",
    "                                    'gin_supervised_edgepred', 'gin_supervised_masking']:\n",
    "        from dgllife.model import GINPredictor\n",
    "        from dgllife.model import load_pretrained\n",
    "        model = GINPredictor(\n",
    "            num_node_emb_list=[120, 3],\n",
    "            num_edge_emb_list=[6, 3],\n",
    "            num_layers=5,\n",
    "            emb_dim=300,\n",
    "            JK=exp_configure['jk'],\n",
    "            dropout=0.5,\n",
    "            readout=exp_configure['readout'],\n",
    "            n_tasks=exp_configure['n_tasks']\n",
    "        )\n",
    "        model.gnn = load_pretrained(exp_configure['model'])\n",
    "        model.gnn.JK = exp_configure['jk']\n",
    "    elif exp_configure['model'] == 'NF':\n",
    "        from dgllife.model import NFPredictor\n",
    "        model = NFPredictor(\n",
    "            in_feats=exp_configure['in_node_feats'],\n",
    "            n_tasks=exp_configure['n_tasks'],\n",
    "            hidden_feats=[exp_configure['gnn_hidden_feats']] * exp_configure['num_gnn_layers'],\n",
    "            batchnorm=[exp_configure['batchnorm']] * exp_configure['num_gnn_layers'],\n",
    "            dropout=[exp_configure['dropout']] * exp_configure['num_gnn_layers'],\n",
    "            predictor_hidden_size=exp_configure['predictor_hidden_feats'],\n",
    "            predictor_batchnorm=exp_configure['batchnorm'],\n",
    "            predictor_dropout=exp_configure['dropout']\n",
    "        )\n",
    "    else:\n",
    "        return ValueError(\"Expect model to be from ['GCN', 'GAT', 'Weave', 'MPNN', 'AttentiveFP', \"\n",
    "                          \"'gin_supervised_contextpred', 'gin_supervised_infomax', \"\n",
    "                          \"'gin_supervised_edgepred', 'gin_supervised_masking', 'NF'], \"\n",
    "                          \"got {}\".format(exp_configure['model']))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "#parser = ArgumentParser('Testing Loading previous Prop Prediction Model for Multi-label Binary Classification')\n",
    "\"\"\"\n",
    "parser.add_argument('-f', '--file-path', type=str, required=True,\n",
    "                    help='Path to a .csv/.txt file of SMILES strings')\n",
    "parser.add_argument('-sc', '--smiles-column', type=str,\n",
    "                    help='Header for the SMILES column in the CSV file, can be '\n",
    "                            'omitted if the input file is a .txt file or the .csv '\n",
    "                            'file only has one column of SMILES strings')\n",
    "parser.add_argument('-tp', '--train-result-path', type=str, default='classification_results',\n",
    "                    help='Path to the saved training results, which will be used for '\n",
    "                            'loading the trained model and related configurations')\n",
    "parser.add_argument('-ip', '--inference-result-path', type=str, default='classification_inference_results',\n",
    "                    help='Path to save the inference results')\n",
    "parser.add_argument('-t', '--task-names', default=None, type=str,\n",
    "                    help='Task names for saving model predictions in the CSV file to output, '\n",
    "                            'which should be the same as the ones used for training. If not '\n",
    "                            'specified, we will simply use task1, task2, ...')\n",
    "parser.add_argument('-s', '--soft-classification', action='store_true', default=False,\n",
    "                    help='By default we will perform hard classification with binary labels. '\n",
    "                            'This flag allows performing soft classification instead.')\n",
    "parser.add_argument('-nw', '--num-workers', type=int, default=1,\n",
    "                    help='Number of processes for data loading (default: 1)')\n",
    "\"\"\"\n",
    "\n",
    "#args = parser.parse_args().__dict__\n",
    "args = {}\n",
    "args['model'] = 'GCN'\n",
    "args['train_result_path'] = 'M2OR_Uniprot_original_GCN'\n",
    "\n",
    "with open('data/configures/M2OR/GCN_canonical.json', 'r') as f:\n",
    "    args.update(json.load(f))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        args['device'] = torch.device('cuda:0')\n",
    "else:\n",
    "        args['device'] = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'GCN',\n",
       " 'train_result_path': 'M2OR_Uniprot_original_GCN',\n",
       " 'lr': 0.02,\n",
       " 'weight_decay': 0,\n",
       " 'patience': 30,\n",
       " 'batch_size': 32,\n",
       " 'dropout': 0.05,\n",
       " 'gnn_hidden_feats': 256,\n",
       " 'predictor_hidden_feats': 128,\n",
       " 'num_gnn_layers': 2,\n",
       " 'residual': True,\n",
       " 'batchnorm': False,\n",
       " 'device': device(type='cuda', index=0)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNPredictor(\n",
       "  (gnn): GCN(\n",
       "    (gnn_layers): ModuleList(\n",
       "      (0): GCNLayer(\n",
       "        (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x7f0cf524b310>)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): GCNLayer(\n",
       "        (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x7f0cf524b310>)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (readout): WeightedSumAndMax(\n",
       "    (weight_and_sum): WeightAndSum(\n",
       "      (atom_weighting): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predict): MLPPredictor(\n",
       "    (predict): Sequential(\n",
       "      (0): Dropout(p=0.05, inplace=False)\n",
       "      (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Linear(in_features=128, out_features=574, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load UniProt GCN model for experiment\n",
    "from dgllife.utils import CanonicalAtomFeaturizer\n",
    "args['node_featurizer'] = CanonicalAtomFeaturizer()\n",
    "args['in_node_feats'] = args['node_featurizer'].feat_size()\n",
    "\"\"\"\n",
    "if args['edge_featurizer'] is not None:\n",
    "    exp_config['in_edge_feats'] = args['edge_featurizer'].feat_size()\n",
    "exp_config.update({\n",
    "    'n_tasks': args['n_tasks'],\n",
    "    'model': args['model']\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "args['n_tasks'] = 574\n",
    "args['device'] = torch.device('cuda:0')\n",
    "\n",
    "model = load_model(args).to(args['device'])\n",
    "checkpoint = torch.load(args['train_result_path'] + '/model.pth', map_location=args['device'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['n_tasks'] = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgllife.model.model_zoo.mlp_predictor import MLPPredictor\n",
    "gnn_out_feats = model.gnn.hidden_feats[-1]\n",
    "model.predict = MLPPredictor(2 * gnn_out_feats, args['predictor_hidden_feats'],\n",
    "                                    args['n_tasks'], dropout = args['dropout'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNPredictor(\n",
       "  (gnn): GCN(\n",
       "    (gnn_layers): ModuleList(\n",
       "      (0): GCNLayer(\n",
       "        (graph_conv): GraphConv(in=74, out=256, normalization=none, activation=<function relu at 0x7f0cf524b310>)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (res_connection): Linear(in_features=74, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): GCNLayer(\n",
       "        (graph_conv): GraphConv(in=256, out=256, normalization=none, activation=<function relu at 0x7f0cf524b310>)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (res_connection): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (readout): WeightedSumAndMax(\n",
       "    (weight_and_sum): WeightAndSum(\n",
       "      (atom_weighting): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predict): MLPPredictor(\n",
       "    (predict): Sequential(\n",
       "      (0): Dropout(p=0.05, inplace=False)\n",
       "      (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgllife",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
